{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8129252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model,load_model, model_from_json\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K \n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2614751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import string\n",
    "from string import digits\n",
    "import re\n",
    "from sklearn.utils import shuffle\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import LSTM, Input, Dense,Embedding, Concatenate, TimeDistributed\n",
    "from tensorflow.keras.models import Model,load_model, model_from_json\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing.text import one_hot, Tokenizer\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from keras import backend as K \n",
    "import pickle as pkl\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30987e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "logger = tf.get_logger()\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    \"\"\"\n",
    "    This class implements Bahdanau attention (https://arxiv.org/pdf/1409.0473.pdf).\n",
    "    There are three sets of weights introduced W_a, U_a, and V_a\n",
    "     \"\"\"\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert isinstance(input_shape, list)\n",
    "        # Create a trainable weight variable for this layer.\n",
    "\n",
    "        self.W_a = self.add_weight(name='W_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.U_a = self.add_weight(name='U_a',\n",
    "                                   shape=tf.TensorShape((input_shape[1][2], input_shape[0][2])),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "        self.V_a = self.add_weight(name='V_a',\n",
    "                                   shape=tf.TensorShape((input_shape[0][2], 1)),\n",
    "                                   initializer='uniform',\n",
    "                                   trainable=True)\n",
    "\n",
    "        super(AttentionLayer, self).build(input_shape)  # Be sure to call this at the end\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"\"\"\n",
    "        inputs: [encoder_output_sequence, decoder_output_sequence]\n",
    "        \"\"\"\n",
    "        assert type(inputs) == list\n",
    "        encoder_out_seq, decoder_out_seq = inputs\n",
    "\n",
    "        logger.debug(f\"encoder_out_seq.shape = {encoder_out_seq.shape}\")\n",
    "        logger.debug(f\"decoder_out_seq.shape = {decoder_out_seq.shape}\")\n",
    "\n",
    "        def energy_step(inputs, states):\n",
    "            \"\"\" Step function for computing energy for a single decoder state\n",
    "            inputs: (batchsize * 1 * de_in_dim)\n",
    "            states: (batchsize * 1 * de_latent_dim)\n",
    "            \"\"\"\n",
    "\n",
    "            logger.debug(\"Running energy computation step\")\n",
    "\n",
    "            if not isinstance(states, (list, tuple)):\n",
    "                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n",
    "\n",
    "            encoder_full_seq = states[-1]\n",
    "\n",
    "            \"\"\" Computing S.Wa where S=[s0, s1, ..., si]\"\"\"\n",
    "            # <= batch size * en_seq_len * latent_dim\n",
    "            W_a_dot_s = K.dot(encoder_full_seq, self.W_a)\n",
    "\n",
    "            \"\"\" Computing hj.Ua \"\"\"\n",
    "            U_a_dot_h = K.expand_dims(K.dot(inputs, self.U_a), 1)  # <= batch_size, 1, latent_dim\n",
    "\n",
    "            logger.debug(f\"U_a_dot_h.shape = {U_a_dot_h.shape}\")\n",
    "\n",
    "            \"\"\" tanh(S.Wa + hj.Ua) \"\"\"\n",
    "            # <= batch_size*en_seq_len, latent_dim\n",
    "            Ws_plus_Uh = K.tanh(W_a_dot_s + U_a_dot_h)\n",
    "\n",
    "            logger.debug(f\"Ws_plus_Uh.shape = {Ws_plus_Uh.shape}\")\n",
    "\n",
    "            \"\"\" softmax(va.tanh(S.Wa + hj.Ua)) \"\"\"\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.squeeze(K.dot(Ws_plus_Uh, self.V_a), axis=-1)\n",
    "            # <= batch_size, en_seq_len\n",
    "            e_i = K.softmax(e_i)\n",
    "\n",
    "            logger.debug(f\"ei.shape = {e_i.shape}\")\n",
    "\n",
    "            return e_i, [e_i]\n",
    "\n",
    "        def context_step(inputs, states):\n",
    "            \"\"\" Step function for computing ci using ei \"\"\"\n",
    "\n",
    "            logger.debug(\"Running attention vector computation step\")\n",
    "\n",
    "            if not isinstance(states, (list, tuple)):\n",
    "                raise TypeError(f\"States must be an iterable. Got {states} of type {type(states)}\")\n",
    "\n",
    "            encoder_full_seq = states[-1]\n",
    "\n",
    "            # <= batch_size, hidden_size\n",
    "            c_i = K.sum(encoder_full_seq * K.expand_dims(inputs, -1), axis=1)\n",
    "\n",
    "            logger.debug(f\"ci.shape = {c_i.shape}\")\n",
    "\n",
    "            return c_i, [c_i]\n",
    "\n",
    "        # we don't maintain states between steps when computing attention\n",
    "        # attention is stateless, so we're passing a fake state for RNN step function\n",
    "        fake_state_c = K.sum(encoder_out_seq, axis=1)\n",
    "        fake_state_e = K.sum(encoder_out_seq, axis=2)  # <= (batch_size, enc_seq_len, latent_dim\n",
    "\n",
    "        \"\"\" Computing energy outputs \"\"\"\n",
    "        # e_outputs => (batch_size, de_seq_len, en_seq_len)\n",
    "        last_out, e_outputs, _ = K.rnn(\n",
    "            energy_step, decoder_out_seq, [fake_state_e], constants=[encoder_out_seq]\n",
    "        )\n",
    "\n",
    "        \"\"\" Computing context vectors \"\"\"\n",
    "        last_out, c_outputs, _ = K.rnn(\n",
    "            context_step, e_outputs, [fake_state_c], constants=[encoder_out_seq]\n",
    "        )\n",
    "\n",
    "        return c_outputs, e_outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        \"\"\" Outputs produced by the layer \"\"\"\n",
    "        return [\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[1][2])),\n",
    "            tf.TensorShape((input_shape[1][0], input_shape[1][1], input_shape[0][1]))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66363f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('Spell_model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "model_loaded = model_from_json(loaded_model_json, custom_objects={'AttentionLayer': AttentionLayer})\n",
    "# load weights into new model\n",
    "model_loaded.load_weights(\"Spell_model_weight.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2660efe",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('inputTokenizer.pkl', 'rb') as f:\n",
    "    inputTokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "579001d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('outputTokenizer.pkl', 'rb') as f:\n",
    "    outputTokenizer = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "806c817d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Eindex2word = inputTokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "07f4cc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mindex2word = outputTokenizer.index_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c3fc12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 34, 300)\n",
      "(None, None)\n"
     ]
    }
   ],
   "source": [
    "latent_dim=300\n",
    "# encoder inference\n",
    "encoder_inputs = model_loaded.input[0]  #loading encoder_inputs\n",
    "encoder_outputs, state_h, state_c = model_loaded.layers[4].output #loading encoder_outputs\n",
    "\n",
    "print(encoder_outputs.shape)\n",
    "\n",
    "encoder_model = Model(inputs=encoder_inputs,outputs=[encoder_outputs, state_h, state_c])\n",
    "\n",
    "# decoder inference\n",
    "# Below tensors will hold the states of the previous time step\n",
    "decoder_state_input_h = Input(shape=(latent_dim,), name=\"decoder_state_input_h\")\n",
    "decoder_state_input_c = Input(shape=(latent_dim,), name=\"decoder_state_input_c\")\n",
    "decoder_hidden_state_input = Input(shape=(34,latent_dim))\n",
    "\n",
    "# Get the embeddings of the decoder sequence\n",
    "decoder_inputs = model_loaded.layers[1].output\n",
    "\n",
    "print(decoder_inputs.shape)\n",
    "dec_emb_layer = model_loaded.layers[3]\n",
    "\n",
    "dec_emb2= dec_emb_layer(decoder_inputs)\n",
    "\n",
    "# To predict the next word in the sequence, set the initial states to the states from the previous time step\n",
    "decoder_lstm = model_loaded.layers[5]\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=[decoder_state_input_h, decoder_state_input_c])\n",
    "\n",
    "#attention inference\n",
    "attn_layer = model_loaded.layers[6]\n",
    "attn_out_inf, attn_states_inf = attn_layer([decoder_hidden_state_input, decoder_outputs2])\n",
    "\n",
    "concate = model_loaded.layers[7]\n",
    "decoder_inf_concat = concate([decoder_outputs2, attn_out_inf])\n",
    "\n",
    "# A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "decoder_dense = model_loaded.layers[8]\n",
    "decoder_outputs2 = decoder_dense(decoder_inf_concat)\n",
    "\n",
    "# Final decoder model\n",
    "decoder_model = Model(\n",
    "[decoder_inputs] + [decoder_hidden_state_input,decoder_state_input_h, decoder_state_input_c],\n",
    "[decoder_outputs2] + [state_h2, state_c2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e7568b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1,1))\n",
    "\n",
    "    # Chose the 'start' word as the first word of the target sequence\n",
    "    target_seq[0, 0] = Mword2index['<']\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + [e_out, e_h, e_c], verbose=0)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "          break\n",
    "        else:\n",
    "          sampled_token = Mindex2word[sampled_token_index]\n",
    "\n",
    "          if(sampled_token!='>'):\n",
    "              decoded_sentence += ''+sampled_token\n",
    "\n",
    "              # Exit condition: either hit max length or find stop word.\n",
    "              if (sampled_token == '>' or len(decoded_sentence.split()) >= (26-1)):\n",
    "                  stop_condition = True\n",
    "\n",
    "          # Update the target sequence (of length 1).\n",
    "          target_seq = np.zeros((1,1))\n",
    "          target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "          # Update internal states\n",
    "          e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79a6f13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2summary(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if((i!=0 and i!=Mword2index['<']) and i!=Mword2index['>']):\n",
    "        newString=newString+Mindex2word[i]+''\n",
    "    return newString\n",
    "\n",
    "def seq2text(input_seq):\n",
    "    newString=''\n",
    "    for i in input_seq:\n",
    "      if(i!=0):\n",
    "        newString=newString+Eindex2word[i]+''\n",
    "    return newString"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5fb14bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test1 = \"emekdaw ile daniwdim dedi 4 iw gununde arawdiralacaq\"\n",
    "test2 = \"ceta yazdi yuklenmedi indi baxiramki pul cixib.\"\n",
    "test3 = \"Hec bir banka odenisim yoxdur\"\n",
    "test4 = \"her bir yerinde gul cicekle bayram\"\n",
    "olmayan1 = \"en gozel ders bu dersdir\"\n",
    "olmayan2 = \"yaxin adam orani qutarib\"\n",
    "olmayan3 = \"gordum seni eve gel\"\n",
    "olmayan4 = \"calis eve tez gel\"\n",
    "olmayan5 = \"meni ora getirme\"\n",
    "olmayan6 = \"basqa sozun var mene\"\n",
    "olmayan7 = \"o ozu bilir\"\n",
    "olmayan8 = \"inanmiram cox islesin\"\n",
    "olmayan9 = \"mence birazdan gelecek\"\n",
    "olmayan10 = \"bu qeder cumle besdi\"\n",
    "\n",
    "#olmayan1 = \"salam. beynelxalqbankin kartina pul kocurmek isteyrem 200 manatdan artıq kocmur gun erzinde\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0901c9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Mword2index = outputTokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "86f51b87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted summary:  inanmıram çox işləsin \n"
     ]
    }
   ],
   "source": [
    "given = olmayan8\n",
    "new_sample = [\"< \" + given +\" >\"]\n",
    "new_sample = np.array(pad_sequences(inputTokenizer.texts_to_sequences(new_sample), maxlen=34, padding='post'))\n",
    "print(\"Predicted summary:\", decode_sequence(new_sample.reshape(1, 34)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5a6fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#predicting 100 unseen sentences\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# read in the dataset from an Excel file\n",
    "df = pd.read_csv('C:/Users/99451/Desktop/sdp_sentences/test_dataset_100_csv.csv')\n",
    "\n",
    "# initialize an empty list to store the predicted summaries\n",
    "summaries = []\n",
    "\n",
    "# loop over each sentence in the dataset\n",
    "for sentence in df['W']:\n",
    "    # preprocess the sentence\n",
    "    sentence = \"< \" + sentence + \" >\"\n",
    "    sentence = np.array(pad_sequences(inputTokenizer.texts_to_sequences([sentence]), maxlen=34, padding='post'))\n",
    "\n",
    "    # generate the summary for the sentence\n",
    "    summary = decode_sequence(sentence.reshape(1, 34))\n",
    "    summaries.append(summary)\n",
    "\n",
    "# add the predicted summaries to the DataFrame\n",
    "df['predicted_summary'] = summaries\n",
    "\n",
    "# save the DataFrame to a new CSV file\n",
    "\n",
    "df.to_csv('test_dataset_100_with_summaries.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c643096c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matching percentage: 73.51%\n"
     ]
    }
   ],
   "source": [
    "#accuracy of predicted true words divided max possibble expected true words\n",
    "\n",
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import Levenshtein\n",
    "\n",
    "# read in the dataset from a CSV file\n",
    "df = pd.read_csv('C:/Users/99451/Desktop/sdp_sentences/test_dataset_100_with_summaries_csv.csv')\n",
    "\n",
    "# initialize a variable to store the total number of sentences\n",
    "total_sentences = len(df)\n",
    "\n",
    "# initialize a variable to store the total number of matched words\n",
    "total_matched_words = 0\n",
    "\n",
    "# initialize a variable to store the total number of possible matched words\n",
    "total_possible_matched_words = 0\n",
    "\n",
    "# loop over each row in the dataset\n",
    "for index, row in df.iterrows():\n",
    "    # get the true sentence and predicted sentence from the row\n",
    "    true_sentence = row['C']\n",
    "    predicted_sentence = row['predicted_summary']\n",
    "    \n",
    "    # split the sentences into words\n",
    "    true_words = true_sentence.split()\n",
    "    predicted_words = predicted_sentence.split()\n",
    "    \n",
    "    # initialize a variable to store the number of matched words in this sentence\n",
    "    matched_words = 0\n",
    "    \n",
    "    # loop over each word in the true sentence\n",
    "    for true_word in true_words:\n",
    "        # loop over each word in the predicted sentence\n",
    "        for predicted_word in predicted_words:\n",
    "            # calculate the Levenshtein distance between the true and predicted words\n",
    "            distance = Levenshtein.distance(true_word, predicted_word)\n",
    "            \n",
    "            # if the distance is 0, the words match\n",
    "            if distance == 0:\n",
    "                # increment the number of matched words and break out of the inner loop\n",
    "                matched_words += 1\n",
    "                break\n",
    "                \n",
    "    # add the number of matched words to the total number of matched words\n",
    "    total_matched_words += matched_words\n",
    "    \n",
    "    # add the length of the true sentence to the total possible matched words\n",
    "    total_possible_matched_words += len(true_words)\n",
    "\n",
    "# calculate the overall matching percentage as a percentage\n",
    "matching_percentage = (total_matched_words / total_possible_matched_words) * 100\n",
    "\n",
    "# print the matching percentage\n",
    "print(f\"Matching percentage: {matching_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a0e4a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word-by-word matching percentage: 72.32%\n"
     ]
    }
   ],
   "source": [
    "#accuracy of expected true and predicted true word by word matching \n",
    "# import the necessary libraries\n",
    "import pandas as pd\n",
    "import Levenshtein\n",
    "\n",
    "# read in the dataset from a CSV file\n",
    "df = pd.read_csv('C:/Users/99451/Desktop/sdp_sentences/test_dataset_100_with_summaries_csv.csv')\n",
    "\n",
    "# initialize a variable to store the total number of sentences\n",
    "total_sentences = len(df)\n",
    "\n",
    "# initialize a variable to store the total number of matching words\n",
    "total_matching_words = 0\n",
    "\n",
    "# initialize a variable to store the total number of words in the true sentences\n",
    "total_true_words = 0\n",
    "\n",
    "# loop over each row in the dataset\n",
    "for index, row in df.iterrows():\n",
    "    # get the true sentence and predicted sentence from the row\n",
    "    true_sentence = row['C']\n",
    "    predicted_sentence = row['predicted_summary']\n",
    "    \n",
    "    # split the sentences into words\n",
    "    true_words = true_sentence.split()\n",
    "    predicted_words = predicted_sentence.split()\n",
    "    \n",
    "    # calculate the number of matching words\n",
    "    matching_words = sum(1 for x, y in zip(true_words, predicted_words) if x == y)\n",
    "    \n",
    "    # update the total number of matching words and true words\n",
    "    total_matching_words += matching_words\n",
    "    total_true_words += len(true_words)\n",
    "\n",
    "# calculate the word-by-word matching percentage\n",
    "word_matching_percentage = (total_matching_words / total_true_words) * 100\n",
    "\n",
    "# print the word-by-word matching percentage\n",
    "print(f\"Word-by-word matching percentage: {word_matching_percentage:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e23da5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CER Accuracy\n",
    "\n",
    "def compute_cer(ref, hyp):\n",
    "    \"\"\"\n",
    "    Compute Character Error Rate (CER) between reference and hypothesis strings.\n",
    "    \"\"\"\n",
    "    # Initialize substitution, deletion and insertion counters to zero\n",
    "    S, D, I = 0, 0, 0\n",
    "    # Compute Levenshtein distance between reference and hypothesis strings\n",
    "    L = [[0] * (len(hyp) + 1) for _ in range(len(ref) + 1)]\n",
    "    for i in range(len(ref) + 1):\n",
    "        L[i][0] = i\n",
    "    for j in range(len(hyp) + 1):\n",
    "        L[0][j] = j\n",
    "    for i in range(1, len(ref) + 1):\n",
    "        for j in range(1, len(hyp) + 1):\n",
    "            cost = (ref[i-1] != hyp[j-1])\n",
    "            L[i][j] = min(L[i-1][j] + 1, L[i][j-1] + 1, L[i-1][j-1] + cost)\n",
    "    # Compute S, D and I by backtracking the Levenshtein matrix\n",
    "    i, j = len(ref), len(hyp)\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and ref[i-1] == hyp[j-1]:\n",
    "            i, j = i - 1, j - 1\n",
    "        elif i > 0 and L[i][j] == L[i-1][j] + 1:\n",
    "            D += 1\n",
    "            i = i - 1\n",
    "        elif j > 0 and L[i][j] == L[i][j-1] + 1:\n",
    "            I += 1\n",
    "            j = j - 1\n",
    "        else:\n",
    "            S += 1\n",
    "            i, j = i - 1, j - 1\n",
    "    # Compute CER as the sum of S, D and I divided by the number of characters in the reference string\n",
    "    cer = (S + D + I) / len(ref)\n",
    "    return cer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d001f697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average CER: 13.84%\n"
     ]
    }
   ],
   "source": [
    "#CER Accuracy\n",
    "import pandas as pd\n",
    "\n",
    "# Load test dataset as a pandas DataFrame with \"W\" and \"C\" columns\n",
    "test_data = pd.read_csv('C:/Users/99451/Desktop/sdp_sentences/test_dataset_100_with_summaries_csv.csv')\n",
    "\n",
    "# Compute CER for each example in the test dataset\n",
    "cer_scores = []\n",
    "for i in range(len(test_data)):\n",
    "    ref = test_data.loc[i, \"C\"]\n",
    "    hyp = test_data.loc[i, \"predicted_summary\"]\n",
    "    cer = compute_cer(ref, hyp)\n",
    "    cer_scores.append(cer)\n",
    "\n",
    "# Compute the average CER score for the test dataset\n",
    "average_cer = sum(cer_scores) / len(cer_scores)\n",
    "print(\"Average CER: {:.2%}\".format(average_cer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fbdfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
